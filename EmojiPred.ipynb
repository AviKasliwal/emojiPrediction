{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emoji Prediction using Transfer Learning\n",
    "\n",
    "Understanding the sentiment of the sentence and then predicting an Emoji for it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports ready!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import emoji\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout, LSTM, Activation\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print (\"Imports ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal of the project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task is to build an Emojifier by using word vector representations. The model will take an input sentence and find the most appropriate emoji to be used with this sentence - from an assortment of 5 emoji's at its disposal.\n",
    "\n",
    "- üòÅ\n",
    "- üòì\n",
    "- üñ§\n",
    "- üç¥\n",
    "- ‚öæ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape = (132, 2), Validation data shape = (56, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>never talk to me again</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I am proud of your achievements</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It is the worst day in my life</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 0  1\n",
       "0           never talk to me again  3\n",
       "1  I am proud of your achievements  2\n",
       "2   It is the worst day in my life  3"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv ('dataset/train_emoji.csv', usecols = [0, 1], header = None)\n",
    "test = pd.read_csv ('dataset/test_emoji.csv', usecols = [0, 1], header = None)\n",
    "\n",
    "print (f'Training data shape = {train.shape}, Validation data shape = {test.shape}')\n",
    "\n",
    "train.head (3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    38\n",
       "3    36\n",
       "0    22\n",
       "1    19\n",
       "4    17\n",
       "Name: 1, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[1].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I want to eat\\t</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>he did not answer\\t</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>he got a raise\\t</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     0  1\n",
       "0      I want to eat\\t  4\n",
       "1  he did not answer\\t  3\n",
       "2     he got a raise\\t  2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head (3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''  \n",
    "    Mapping label to a emoji\n",
    "'''\n",
    "\n",
    "emoji_dictionary = {\"0\": \"\\u2764\\uFE0F\",    # üñ§\n",
    "                    \"1\": \":baseball:\", # ‚öæ\n",
    "                    \"2\": \":beaming_face_with_smiling_eyes:\", # üòÅ\n",
    "                    \"3\": \":downcast_face_with_sweat:\", # üòì\n",
    "                    \"4\": \":fork_and_knife:\", # üç¥\n",
    "                   }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Getting x_train, y_train from train.csv\n",
    "    x_test, y_test from test.csv\n",
    "'''\n",
    "\n",
    "x_train = train.values[:, 0]\n",
    "y_train = to_categorical (train.values[:, 1])\n",
    "\n",
    "x_test = test.values[:, 0]\n",
    "y_test = to_categorical (test.values[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of longest sentence (by number of words) is : 10\n"
     ]
    }
   ],
   "source": [
    "maxLen = 0 # Len of longest sentence (by number of words)\n",
    "\n",
    "for sent in x_train:\n",
    "    maxLen = max (maxLen, len (sent.split (' ')))\n",
    "\n",
    "for sent in x_test:\n",
    "    maxLen = max (maxLen, len (sent.split (' ')))\n",
    "    \n",
    "print (f\"Length of longest sentence (by number of words) is : {maxLen}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence : never talk to me again , Emoji : üòì\n",
      "Sentence : I am proud of your achievements , Emoji : üòÅ\n",
      "Sentence : It is the worst day in my life , Emoji : üòì\n",
      "Sentence : Miss you so much , Emoji : ‚ù§Ô∏è\n",
      "Sentence : food is life , Emoji : üç¥\n",
      "Sentence : I love you mum , Emoji : ‚ù§Ô∏è\n",
      "Sentence : Stop saying bullshit , Emoji : üòì\n",
      "Sentence : congratulations on your acceptance , Emoji : üòÅ\n",
      "Sentence : The assignment is too long  , Emoji : üòì\n",
      "Sentence : I want to go play , Emoji : ‚öæ\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    Frst 10 training points\n",
    "'''\n",
    "\n",
    "for i, sent in enumerate (x_train):\n",
    "    if i == 10:\n",
    "        break\n",
    "    label = str (np.argmax (y_train[i]))\n",
    "    print (f\"Sentence : {sent} , Emoji : {emoji.emojize (emoji_dictionary[label])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating word embeddings for the words that are present in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll be using word vector representations of the words in the sentence so I need word vector representations of the words in the sentences. I'll use the Glove vectors for this representation.                     \n",
    "                \n",
    "Based on few iterations 100 d vectors seem to work best for this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "embeddings dictionary:\n",
    "    key = word\n",
    "    value = embedding vector [100 dimension vector]\n",
    "'''\n",
    "\n",
    "embeddings = {}\n",
    "with open ('glove.6B.100d.txt', encoding = 'utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split () # splits the word and coeff\n",
    "        word = values[0] # word\n",
    "        coeffs = np.asarray (values[1 : ], dtype = 'float32') # makes a word vector of len 50 for each word\n",
    "        embeddings[word] = coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getOutputEmbeddings(X):\n",
    "    \n",
    "    embedding_matrix_output = np.zeros ((X.shape[0], 10, 100)) # X.shape (num_of_sentences, max_len, embedding_dim)\n",
    "    for ix in range (X.shape[0]): # go to every sentence\n",
    "        X[ix] = X[ix].split () # get a list of words of the sentence\n",
    "        for jx in range (len(X[ix])): # go to every word\n",
    "            embedding_matrix_output[ix][jx] = embeddings[X[ix][jx].lower ()]\n",
    "            \n",
    "    return embedding_matrix_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((132, 10, 100), (56, 10, 100))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_x_train = getOutputEmbeddings(x_train) # getting embeddings for train data\n",
    "emb_x_test = getOutputEmbeddings(x_test) # getting embeddings for test data\n",
    "\n",
    "emb_x_train.shape, emb_x_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 10, 128)           117248    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 10, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 5)                 645       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 249,477\n",
      "Trainable params: 249,477\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add (LSTM (128, input_shape = (10, 100),return_sequences=True)) # to create a stacked LSTM model\n",
    "model.add (Dropout (0.5)) # regularisation\n",
    "model.add (LSTM (128, input_shape = (10, 100))) # stacking LSTM layer\n",
    "model.add (Dropout (0.5))\n",
    "model.add (Dense (5))\n",
    "model.add (Activation ('softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Callbacks'''\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor = 'loss', factor = 0.3, patience = 3, min_lr = 0.0001, verbose = 1)\n",
    "checkpoint = ModelCheckpoint (\"best_model.h5\", monitor = 'val_acc', verbose = True, save_best_only = True)\n",
    "\n",
    "callbacks = [reduce_lr, checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Automatic model reloading for interrupted job was removed from the `ModelCheckpoint` callback in multi-worker mode, please use the `keras.callbacks.experimental.BackupAndRestore` callback instead. See this tutorial for details: https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras#backupandrestore_callback.\n",
      "Epoch 1/25\n",
      "6/6 [==============================] - ETA: 0s - loss: 1.6126 - acc: 0.2543\n",
      "Epoch 00001: val_acc improved from -inf to 0.35000, saving model to best_model.h5\n",
      "6/6 [==============================] - 7s 1s/step - loss: 1.6090 - acc: 0.2568 - val_loss: 1.5709 - val_acc: 0.3500\n",
      "Epoch 2/25\n",
      "6/6 [==============================] - ETA: 0s - loss: 1.5456 - acc: 0.3066\n",
      "Epoch 00002: val_acc did not improve from 0.35000\n",
      "6/6 [==============================] - 0s 47ms/step - loss: 1.5369 - acc: 0.3156 - val_loss: 1.5034 - val_acc: 0.3500\n",
      "Epoch 3/25\n",
      "6/6 [==============================] - ETA: 0s - loss: 1.4059 - acc: 0.4240\n",
      "Epoch 00003: val_acc improved from 0.35000 to 0.40000, saving model to best_model.h5\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 1.4004 - acc: 0.4256 - val_loss: 1.4751 - val_acc: 0.4000\n",
      "Epoch 4/25\n",
      "6/6 [==============================] - ETA: 0s - loss: 1.2822 - acc: 0.5364\n",
      "Epoch 00004: val_acc improved from 0.40000 to 0.62500, saving model to best_model.h5\n",
      "6/6 [==============================] - 0s 44ms/step - loss: 1.2781 - acc: 0.5374 - val_loss: 1.2530 - val_acc: 0.6250\n",
      "Epoch 5/25\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 1.0890 - acc: 0.7067\n",
      "Epoch 00005: val_acc improved from 0.62500 to 0.67500, saving model to best_model.h5\n",
      "6/6 [==============================] - 0s 65ms/step - loss: 1.0820 - acc: 0.7035 - val_loss: 1.0213 - val_acc: 0.6750\n",
      "Epoch 6/25\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.9220 - acc: 0.6780\n",
      "Epoch 00006: val_acc did not improve from 0.67500\n",
      "6/6 [==============================] - 0s 63ms/step - loss: 0.9257 - acc: 0.6743 - val_loss: 1.0447 - val_acc: 0.6500\n",
      "Epoch 7/25\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.7920 - acc: 0.6981\n",
      "Epoch 00007: val_acc did not improve from 0.67500\n",
      "6/6 [==============================] - 0s 55ms/step - loss: 0.7871 - acc: 0.7024 - val_loss: 0.9118 - val_acc: 0.6000\n",
      "Epoch 8/25\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.6041 - acc: 0.8326\n",
      "Epoch 00008: val_acc did not improve from 0.67500\n",
      "6/6 [==============================] - 0s 60ms/step - loss: 0.5927 - acc: 0.8348 - val_loss: 0.9444 - val_acc: 0.6500\n",
      "Epoch 9/25\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3753 - acc: 0.9097\n",
      "Epoch 00009: val_acc did not improve from 0.67500\n",
      "6/6 [==============================] - 0s 46ms/step - loss: 0.3824 - acc: 0.9070 - val_loss: 1.0375 - val_acc: 0.6750\n",
      "Epoch 10/25\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.4306 - acc: 0.8689\n",
      "Epoch 00010: val_acc did not improve from 0.67500\n",
      "6/6 [==============================] - 0s 41ms/step - loss: 0.4262 - acc: 0.8690 - val_loss: 1.1589 - val_acc: 0.6250\n",
      "Epoch 11/25\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.3399 - acc: 0.9123\n",
      "Epoch 00011: val_acc did not improve from 0.67500\n",
      "6/6 [==============================] - 0s 59ms/step - loss: 0.3329 - acc: 0.9125 - val_loss: 1.1515 - val_acc: 0.6500\n",
      "Epoch 12/25\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.2828 - acc: 0.9033\n",
      "Epoch 00012: val_acc did not improve from 0.67500\n",
      "6/6 [==============================] - 0s 54ms/step - loss: 0.2751 - acc: 0.9092 - val_loss: 1.4164 - val_acc: 0.6750\n",
      "Epoch 13/25\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.3115 - acc: 0.9056\n",
      "Epoch 00013: val_acc improved from 0.67500 to 0.75000, saving model to best_model.h5\n",
      "6/6 [==============================] - 0s 60ms/step - loss: 0.3084 - acc: 0.8984 - val_loss: 1.4243 - val_acc: 0.7500\n",
      "Epoch 14/25\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.1903 - acc: 0.9337\n",
      "Epoch 00014: val_acc did not improve from 0.75000\n",
      "6/6 [==============================] - 0s 54ms/step - loss: 0.1796 - acc: 0.9403 - val_loss: 1.3824 - val_acc: 0.6000\n",
      "Epoch 15/25\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.1378 - acc: 0.9571\n",
      "Epoch 00015: val_acc did not improve from 0.75000\n",
      "6/6 [==============================] - 0s 56ms/step - loss: 0.1378 - acc: 0.9555 - val_loss: 1.2226 - val_acc: 0.6250\n",
      "Epoch 16/25\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.1171 - acc: 0.9640\n",
      "Epoch 00016: val_acc did not improve from 0.75000\n",
      "6/6 [==============================] - 0s 46ms/step - loss: 0.1156 - acc: 0.9645 - val_loss: 1.1917 - val_acc: 0.6250\n",
      "Epoch 17/25\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.0847 - acc: 0.9840\n",
      "Epoch 00017: val_acc did not improve from 0.75000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 0.0805 - acc: 0.9854 - val_loss: 1.2759 - val_acc: 0.6250\n",
      "Epoch 18/25\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.0525 - acc: 1.0000\n",
      "Epoch 00018: val_acc did not improve from 0.75000\n",
      "6/6 [==============================] - 0s 55ms/step - loss: 0.0493 - acc: 1.0000 - val_loss: 1.3710 - val_acc: 0.6250\n",
      "Epoch 19/25\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.0184 - acc: 1.0000\n",
      "Epoch 00019: val_acc did not improve from 0.75000\n",
      "6/6 [==============================] - 0s 54ms/step - loss: 0.0186 - acc: 1.0000 - val_loss: 1.5391 - val_acc: 0.6250\n",
      "Epoch 20/25\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.0277 - acc: 1.0000\n",
      "Epoch 00020: val_acc did not improve from 0.75000\n",
      "6/6 [==============================] - 0s 56ms/step - loss: 0.0265 - acc: 1.0000 - val_loss: 1.7301 - val_acc: 0.6000\n",
      "Epoch 21/25\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.0114 - acc: 1.0000\n",
      "Epoch 00021: val_acc did not improve from 0.75000\n",
      "6/6 [==============================] - 0s 43ms/step - loss: 0.0116 - acc: 1.0000 - val_loss: 1.8478 - val_acc: 0.6000\n",
      "Epoch 22/25\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.0146 - acc: 1.0000\n",
      "Epoch 00022: val_acc did not improve from 0.75000\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 0.0139 - acc: 1.0000 - val_loss: 1.8922 - val_acc: 0.6250\n",
      "Epoch 23/25\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.0355 - acc: 0.9715\n",
      "Epoch 00023: val_acc did not improve from 0.75000\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 0.0320 - acc: 0.9765 - val_loss: 1.9532 - val_acc: 0.5750\n",
      "Epoch 24/25\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.0588 - acc: 0.9817\n",
      "Epoch 00024: val_acc did not improve from 0.75000\n",
      "6/6 [==============================] - 0s 45ms/step - loss: 0.0694 - acc: 0.9797 - val_loss: 1.8632 - val_acc: 0.6000\n",
      "Epoch 25/25\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.2034 - acc: 0.9398\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.75000\n",
      "6/6 [==============================] - 0s 43ms/step - loss: 0.3125 - acc: 0.9197 - val_loss: 1.8771 - val_acc: 0.6000\n"
     ]
    }
   ],
   "source": [
    "model.compile (optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['acc'])\n",
    "hist = model.fit (emb_x_train, y_train, epochs = 25, batch_size = 16, shuffle = True, validation_split = 0.3, callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 21ms/step - loss: 1.1657 - acc: 0.5893\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.1656949520111084, 0.5892857313156128]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights ('best_model.h5')\n",
    "model.evaluate (emb_x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict_classes(emb_x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence      : Actual       Prediction\n",
      "I want to eat : üç¥ \t\tüç¥\n",
      "he did not answer : üòì \t\tüòì\n",
      "he got a raise : üòÅ \t\tüòì\n",
      "she got me a present : ‚ù§Ô∏è \t\tüòì\n",
      "ha ha ha it was so funny : üòÅ \t\tüòÅ\n",
      "he is a good friend : ‚ù§Ô∏è \t\tüòì\n",
      "I am upset : ‚ù§Ô∏è \t\tüòì\n",
      "We had such a lovely dinner tonight : ‚ù§Ô∏è \t\tüòì\n",
      "where is the food : üç¥ \t\tüç¥\n",
      "Stop making this joke ha ha ha : üòÅ \t\tüòÅ\n"
     ]
    }
   ],
   "source": [
    "print(\"Sentence      : Actual       Prediction\")\n",
    "for i in range(10):\n",
    "    print(' '.join(x_test[i]), end = \" : \")\n",
    "    print(emoji.emojize(emoji_dictionary[str(np.argmax(y_test[i]))]), end = \" \\t\\t\")\n",
    "    print(emoji.emojize(emoji_dictionary[str(pred[i])]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am upset : ‚ù§Ô∏è \t\tüòì                 \n",
    "We had such a lovely dinner tonight : ‚ù§Ô∏è \t\tüòÅ            \n",
    "    \n",
    "for these sentences our predictions were also good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "    Confusion Matrix\n",
    "'''\n",
    "\n",
    "Y_test = [np.argmax (i) for i in y_test] \n",
    "\n",
    "cm = confusion_matrix(Y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(cm, annot = True)\n",
    "plt.show ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks for future"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Get more data\n",
    "- Try different model architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
